version: "3"
services:
  # https://raw.githubusercontent.com/bitnami/containers/main/bitnami/spark/docker-compose.yml
  spark:
    image: bitnami/spark:3.3-debian-11
    container_name: spark
    environment:
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - KAFKA_HOST=kafka:29092
      - PYSPARK_SUBMIT_ARGS=--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1 pyspark-shell
    ports:
      - '8080:8080'
      - '7077:7077'
      - '41573:41573'
  spark-worker:
    # image: docker.io/bitnami/spark:3.3
    # build: Dockerfile.spark
    container_name: spark-worker
    build:
      dockerfile: Dockerfile.spark
    ports:
      - '8091:8081'
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_WORKER_MEMORY=16G
      - SPARK_WORKER_CORES=4
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - KAFKA_HOST=kafka:29092
      - PYSPARK_SUBMIT_ARGS=--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1 pyspark-shell
    depends_on:
      - spark

  # https://github.com/bitnami/containers/blob/main/bitnami/kafka/docker-compose.yml
  zookeeper:
    image: docker.io/bitnami/zookeeper:3.8
    ports:
      - "2181:2181"
    # volumes:
    #   - "zookeeper_data:/bitnami"
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes

  kafka:
    image: docker.io/bitnami/kafka:3.3
    container_name: kafka
    ports:
      - "9092:9092"
    # volumes:
    #   - "kafka_data:/bitnami"
    environment:
      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      - KAFKA_LISTENERS=PLAINTEXT://kafka:29092,CONTROLLER://kafka:29093,PLAINTEXT_HOST://0.0.0.0:9092
    depends_on:
      - zookeeper

  kafka-ui:
    container_name: kafka-ui
    image: provectuslabs/kafka-ui:latest
    ports:
      - 8081:8080
    depends_on:
      - kafka
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092

  # cassandra:
  #   container_name: cassandra
  #   image: cassandra:4.1.0
  #   ports:
  #     - 7000:7000

        # hadoop

  namenode:
    image: m8928/hadoop-namenode:3.2.2-java8
    # image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: always
    ports:
      - 9870:9870
      # - 9000:9000
      - 9000:8020
      - 50070:50070
      - 8020:8020
    volumes:
      # - /data/docker_container/hadoop_namenode:/hadoop/dfs/name
      # - /tmp:/tmp
      - hadoop_namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
      # - HADOOP_CLASSPATH=${JAVA_HOME}/lib/tools.jar
      - HADOOP_CLASSPATH=/usr/lib/jvm/java-8-openjdk-amd64/lib/tools.jar
    env_file:
      - ./docker-hadoop/hadoop.env

  datanode01:
    image: m8928/hadoop-datanode:3.2.2-java8
    container_name: datanode01
    restart: always
    volumes:
      # - /data/docker_container/hadoop_datanode01:/hadoop/dfs/data
      - hadoop_datanode01:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./docker-hadoop/hadoop.env

  # datanode02:
  #   image: m8928/hadoop-datanode:3.2.2-java8
  #   container_name: datanode02
  #   restart: always
  #   volumes:
  #     # - /data/docker_container/hadoop_datanode02:/hadoop/dfs/data
  #     - hadoop_datanode02:/hadoop/dfs/data
  #   environment:
  #     SERVICE_PRECONDITION: "namenode:9870"
  #   env_file:
  #     - ./docker-hadoop/hadoop.env

  # datanode03:
  #   image: m8928/hadoop-datanode:3.2.2-java8
  #   container_name: datanode03
  #   restart: always
  #   volumes:
  #     # - /data/docker_container/hadoop_datanode03:/hadoop/dfs/data
  #     - hadoop_datanode03:/hadoop/dfs/data
  #   environment:
  #     SERVICE_PRECONDITION: "namenode:9870"
  #   env_file:
  #     - ./docker-hadoop/hadoop.env

  resourcemanager:
    # image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
    image: m8928/hadoop-resourcemanager:3.2.2-java8
    container_name: resourcemanager
    restart: always
    expose:
      - 8031
      - 8030
    ports:
      - 18088:8088
      - 8032:8032
      - 8088:8088
    environment:
      # SERVICE_PRECONDITION: "namenode:8020 namenode:9870 datanode01:9864 datanode02:9864 datanode03:9864"
      SERVICE_PRECONDITION: "namenode:8020 namenode:9870 datanode01:9864"
    env_file:
      - ./docker-hadoop/hadoop.env
    depends_on:
      - datanode01
      # - datanode02
      # - datanode03

  nodemanager01:
    image: m8928/hadoop-nodemanager:3.2.2-java8
    container_name: nodemanager01
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:8020 namenode:9870 datanode01:9864 resourcemanager:8088"
    env_file:
      - ./docker-hadoop/hadoop.env
    depends_on:
      - resourcemanager

  # nodemanager02:
  #   image: m8928/hadoop-nodemanager:3.2.2-java8
  #   container_name: nodemanager02
  #   restart: always
  #   environment:
  #     SERVICE_PRECONDITION: "namenode:8020 namenode:9870 datanode02:9864 resourcemanager:8088"
  #   env_file:
  #     - ./docker-hadoop/hadoop.env
  #   depends_on:
  #     - resourcemanager

  # nodemanager03:
  #   image: m8928/hadoop-nodemanager:3.2.2-java8
  #   container_name: nodemanager03
  #   restart: always
  #   environment:
  #     SERVICE_PRECONDITION: "namenode:8020 namenode:9870 datanode03:9864 resourcemanager:8088"
  #   env_file:
  #     - ./docker-hadoop/hadoop.env
  #   depends_on:
  #     - resourcemanager

  historyserver:
    image: m8928/hadoop-historyserver:3.2.2-java8
    container_name: historyserver
    restart: always
    ports:
      - 18188:8188
      - 8188:8188
    environment:
      # SERVICE_PRECONDITION: "namenode:8020 namenode:9870 datanode01:9864 datanode02:9864 datanode03:9864 resourcemanager:8088"
      SERVICE_PRECONDITION: "namenode:8020 namenode:9870 datanode01:9864 resourcemanager:8088"
    volumes:
      # - /data/docker_container/hadoop_historyserver:/hadoop/yarn/timeline
      - hadoop_historyserver:/hadoop/yarn/timeline
    env_file:
      - ./docker-hadoop/hadoop.env

  # metastore:
  #   image: mariadb:latest
  #   container_name: metastore
  #   restart: always
  #   ports:
  #     - 23306:3306
  #   environment:
  #     MYSQL_ROOT_PASSWORD: "!gl00sec"
  #     MYSQL_DATABASE: "hive"
  #     MYSQL_USER: "hive"
  #     MYSQL_PASSWORD: "!gl00sec"
  metastore:
    image: postgres:latest
    expose:
      - 5432
    volumes:
      - postgresql:/var/lib/postgresql/data/pgdata
    environment:
      POSTGRES_PASSWORD: "hive"
      POSTGRES_USER: "hive"
      POSTGRES_DB: "hive"
      PGDATA: "/var/lib/postgresql/data/pgdata"

  hiveserver:
    image: m8928/hadoop-hiveserver:3.2.2-java8
    # entrypoint: ["sleep", "300"]
    container_name: hiveserver
    # restart: never
    ports:
      - 20000:10000
      - 20002:10002
    environment:
      SERVICE_PRECONDITION: "namenode:8020 namenode:9870 metastore:5432"
      HADOOP_HOME: /opt/hadoop-3.3.4
      HIVE_CONF_DIR: /opt/hive/conf
      # PATH: $PATH:$HADOOP_HOME/bin:$HIVE_HOME/bin

      MAPRED_CONF_yarn_app_mapreduce_am_env: HADOOP_MAPRED_HOME=/opt/hadoop-3.3.4
      MAPRED_CONF_mapreduce_map_env: HADOOP_MAPRED_HOME=/opt/hadoop-3.3.4
      MAPRED_CONF_mapreduce_reduce_env: HADOOP_MAPRED_HOME=/opt/hadoop-3.3.4
    env_file:
      - ./docker-hadoop/hadoop.env

volumes:
  hadoop_namenode:
  hadoop_datanode01:
  hadoop_datanode02:
  hadoop_datanode03:
  hadoop_historyserver:
  postgresql: